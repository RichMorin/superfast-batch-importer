package org.neo4j.batchimport;

import org.neo4j.batchimport.importer.CsvLineData;
import org.neo4j.batchimport.importer.RelType;
import org.neo4j.batchimport.importer.Type;
import org.neo4j.batchimport.newimport.ReadFileData;
import org.neo4j.batchimport.index.MapDbCachingIndexProvider;
import org.neo4j.batchimport.newimport.DataChunker;
import org.neo4j.batchimport.utils.Config;
import org.neo4j.batchimport.newimport.DataBuffers.*;
import org.neo4j.batchimport.newimport.structs.CSVDataBuffer;
import org.neo4j.batchimport.newimport.structs.RunData;
import org.neo4j.batchimport.newimport.utils.Utils;
import org.neo4j.graphdb.DynamicLabel;
import org.neo4j.graphdb.Label;
import org.neo4j.graphdb.index.IndexManager;
import org.neo4j.index.lucene.unsafe.batchinsert.LuceneBatchInserterIndexProvider;
import org.neo4j.kernel.impl.nioneo.store.NeoStore;
import org.neo4j.kernel.impl.nioneo.store.NodeRecord;
import org.neo4j.kernel.impl.nioneo.store.RelationshipRecord;
import org.neo4j.kernel.impl.util.FileUtils;
import org.neo4j.unsafe.batchinsert.BatchInserter;
import org.neo4j.unsafe.batchinsert.BatchInserters;
import org.neo4j.unsafe.batchinsert.BatchInserterIndexProvider;
import org.neo4j.unsafe.batchinsert.BatchInserterIndex;
import org.neo4j.unsafe.batchinsert.BatchInserterImpl;
import org.neo4j.unsafe.batchinsert.NewBatchInserterImpl;

import java.io.*;
import java.text.SimpleDateFormat;
import java.net.*;
import java.sql.Timestamp;
import java.util.jar.*;
import java.util.*;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.Callable;
import java.util.concurrent.TimeUnit;
import java.util.zip.GZIPInputStream;
import java.lang.reflect.Method;
import java.lang.instrument.Instrumentation;

import static org.neo4j.batchimport.Utils.join;
import static org.neo4j.index.impl.lucene.LuceneIndexImplementation.EXACT_CONFIG;
import static org.neo4j.index.impl.lucene.LuceneIndexImplementation.FULLTEXT_CONFIG;

import org.neo4j.consistency.ConsistencyCheckTool;


public class NewImporter {
    private static final Map<String, String> SPATIAL_CONFIG = Collections.singletonMap(IndexManager.PROVIDER,"spatial");
    private static final Label[] NO_LABELS = new Label[0];
    public static final int BATCH =  1 * 1000 * 1000;
    private static Report report;
    private final Config config;
    private BatchInserterIndexProvider indexProvider;
    Map<String,BatchInserterIndex> indexes=new HashMap<String, BatchInserterIndex>();
    private Label[] labelsArray = NO_LABELS;
    //----
	public static boolean kernel_import_opt = false;
	public static boolean new_parallel_import_node = true;
	public static boolean new_parallel_import_rel = true;
	public static ThreadLocal<DataBuffer> threadLocal = new ThreadLocal<DataBuffer>();
	public static ThreadLocal<RunData> threadLocalPerf = new ThreadLocal<RunData>();
    public static int BUFFER_SIZE_BYTES = 320 *1024;//314566;
    public static int BUFFER_ENTRIES = 4000;
    public static int BUFFERQ_SIZE = 32;
    public static int BUFFER_READERS = 3;
    public static int pollInterval = 10000;
    public static boolean detailedData = false;
    private static long startImport = 0;
    
    private BatchInserterImpl batchInserter;
    private NewBatchInserterImpl db;

    public NewImporter(File graphDb, final Config config) {
        this.config = config;
		Timestamp ts = new Timestamp(System.currentTimeMillis());
        System.out.println("[Current time:"+ts.toString()+"][Compile Time:"+this.getVersionfinal(this.getClass())+"]");
        batchInserter = createBatchInserter(graphDb, config);
        db = new NewBatchInserterImpl(batchInserter);

        final boolean luceneOnlyIndex = config.isCachedIndexDisabled();
        indexProvider = createIndexProvider(luceneOnlyIndex);
        Collection<IndexInfo> indexInfos = config.getIndexInfos();
        if (indexInfos!=null) {
            for (IndexInfo indexInfo : indexInfos) {
                BatchInserterIndex index = indexInfo.isNodeIndex() ? nodeIndexFor(indexInfo.indexName, indexInfo.indexType) : relationshipIndexFor(indexInfo.indexName, indexInfo.indexType);
                indexes.put(indexInfo.indexName, index);
                //index.setCommitBatchSize(1000000);
            }
        }

        report = createReport();
    }

    public String getVersionfinal (Class classe) {
	String version = null;
	String shortClassName = classe.getName().substring(classe.getName().lastIndexOf(".") + 1);
	try {
		ClassLoader cl = this.getClass().getClassLoader();
		String threadContexteClass = classe.getName().replace('.', '/');
		URL url = cl.getResource(threadContexteClass + ".class");
		if ( url == null ) {
			version = shortClassName + " $ (no manifest)";
		} else {
			String path = url.getPath();
			String jarExt = ".jar";
			int index = path.indexOf(jarExt);
			SimpleDateFormat sdf = new SimpleDateFormat("dd/MM/yyyy HH:mm:ss");
			if (index != -1) {
				String jarPath = path.substring(0, index + jarExt.length());
				File file = new File(jarPath);
				String jarVersion = file.getName();
				JarFile jarFile = new JarFile(new File(new URI(jarPath)));
				JarEntry entry = jarFile.getJarEntry("META-INF/MANIFEST.MF");
				version = shortClassName + " $ " + jarVersion.substring(0, jarVersion.length()
						- jarExt.length()) + " $ "
						+ sdf.format(new Date(entry.getTime()));
				jarFile.close();
			} else {
				File file = new File(path);
				version = shortClassName + " $ " + sdf.format(new Date(file.lastModified()));
			}
		}
	} catch (Exception e) {
		version = shortClassName + " $ " + e.toString();
	}
	return version;
    }
    protected StdOutReport createReport() {
        return new StdOutReport(BATCH, 1);
    }

    protected BatchInserterIndexProvider createIndexProvider(boolean luceneOnlyIndex) {
        return luceneOnlyIndex ? new LuceneBatchInserterIndexProvider(batchInserter) : new MapDbCachingIndexProvider(batchInserter);
    }

    protected BatchInserterImpl createBatchInserter(File graphDb, Config config) {
       return (BatchInserterImpl)BatchInserters.inserter(graphDb.getAbsolutePath(), config.getConfigData());
    	//return new BatchInserterImpl(graphDb.getAbsolutePath(), config.getConfigData());
    }
    static private HashMap<String, String> processNewArgs(String[] args){
		if (!args[0].contains("="))
			return null;
		HashMap<String, String> argsMap = new HashMap<String, String>();
		for (String arg : args){
        	String[] argParts = arg.split("=");
        	if (argParts.length == 2)
        		argsMap.put(argParts[0], argParts[1]);
        	else 
        		argsMap.put(arg, null);
        }
		return argsMap;
    }

    // todo multiple nodes and rels files
    // todo nodes and rels-files in config
    // todo graphdb in config
    public static void main(String[] args) throws IOException {
    	startImport= System.currentTimeMillis();
    	if (args[0].equalsIgnoreCase("ConsistencyCheck")){
    		ConsistencyCheckTool.main(new String[]{args[1]});
    	} else {
    		System.err.println("Usage: NewImporter data/dir nodes.csv relationships.csv [node_index node-index-name fulltext|exact nodes_index.csv rel_index rel-index-name fulltext|exact rels_index.csv ....]");
    		System.err.println("Using: NewImporter "+join(args," "));
    		System.err.println();
    		for (int i = 0; i < args.length; i++){
    			if (args[i].startsWith("new_parallel_import")){
    				String[] parts = args[i].split("=");
    				if (parts.length > 1)
    					maxNodes = Integer.parseInt(parts[1]);
    				new_parallel_import_node = true;
    				new_parallel_import_rel = true;
    				args[i] = null;
    			}				
    		}
    		args = getMaxNodes(args);
    		final Config config = Config.convertArgumentsToConfig(args);

    		File graphDb = new File(config.getGraphDbDirectory());
    		if (graphDb.exists() && !config.keepDatabase()) {
    			FileUtils.deleteRecursively(graphDb);
    		}
    		
    		NewImporter importer = new NewImporter(graphDb, config);
    		importer.doImport();
    	}
    	System.out.println("Total time taken: "+(System.currentTimeMillis()-startImport));
    }

    public static int maxNodes = -1; 
    private static String[] getMaxNodes(String[] args){
    	ArrayList<String> argsList = new ArrayList();
    	for (int i = 0; i < args.length; i++){
    		if (args[i] != null && args[i].startsWith("maxNodes")){
    			String count = args[i].split("=")[1];
    			maxNodes = Integer.parseInt(count.trim());
    			continue;
    		}
    		if (args[i] != null) argsList.add(args[i]);
    	}
    	return argsList.toArray(new String[0]);
    }
    void finish() {
        indexProvider.shutdown();
        batchInserter.shutdown();
        report.finish();
    }

    void importNodes(BufferedReader reader) throws IOException {
        final LineData data = createLineData(reader, 0);
        report.reset();
        boolean hasId = data.hasId();
        while (data.processLine(null)) {
            String[] labels = data.getTypeLabels();
            long id;
            if (hasId) {
                id = data.getId();
                batchInserter.createNode(id, data.getProperties(),labelsFor(labels));
            } else {
                id = batchInserter.createNode(data.getProperties(),labelsFor(labels));
            }
            for (Map.Entry<String, Map<String, Object>> entry : data.getIndexData().entrySet()) {
                final BatchInserterIndex index = indexFor(entry.getKey());
                if (index==null)
                    throw new IllegalStateException("Index "+entry.getKey()+" not configured.");
                index.add(id, entry.getValue());
            }
            report.dots();

            if (report.getCount() % BATCH == 0) flushIndexes();
        }
        flushIndexes();
        report.finishImport("Nodes");
    }
    //+++++++++++++
    private NewArrayBlockingQ bufferQ;
    private DiskRecordsCache diskRecordsCache;
    //public static ArrayBlockingQueue<DiskRecords>[] diskRecordsQ = new ArrayBlockingQueue[3];
    public static NewDiskBlockingQ diskRecordsQ = new NewDiskBlockingQ(3, NewImporter.BUFFERQ_SIZE);
    //======================== Parallel node ================
    private boolean[] stageComplete; 
    private RunData[][] stageRunData;
    private RunData[] writerRunData;
    public static int IMPORT_NODE_STAGES = 4;
    public static int IMPORT_NODE_THREADS = Runtime.getRuntime().availableProcessors();//IMPORT_NODE_STAGES*4;
    static long waitTime_importNodes = 0;
    static long processTime_importNodes = 0;
    public static int startEMPTYQ_SIZE = 0;
    private ImportWorker[] importNodeWorkers = new ImportWorker[IMPORT_NODE_THREADS];
    private boolean[] singleThreadedStages = new boolean[]{false, true, false, false, false, true};
    
    public DataBuffer[] createInputBufferArray(int size){
    	DataBuffer[] bufferArray = new DataBuffer[size];
    	for (int i = 0; i < size; i++){
    		DataBuffer buf = new DataBuffer(NewImporter.BUFFER_ENTRIES, NewImporter.BUFFER_SIZE_BYTES, i, diskRecordsCache);
    		bufferArray[i] = buf;
    	};
    	return bufferArray;
    }
    private void initInputQ(ArrayBlockingQueue<DataBuffer> inputQ){
    	int size = inputQ.remainingCapacity();
    	try {
        	for (int i = 0; i < size; i++)
        		inputQ.put( bufferArray[i]);
        	System.out.println("EmptyQ Size:"+ inputQ.size());
        	NewImporter.startEMPTYQ_SIZE = inputQ.size();
        } catch (InterruptedException ie){
        	Utils.SystemOutPrintln("Interruped empty queue creation");
        } 
    }
    
    private void prepareForParallel(){
    	long mem1 = Runtime.getRuntime().freeMemory();
    	diskRecordsCache = new DiskRecordsCache(BUFFERQ_SIZE*2, NewImporter.BUFFER_ENTRIES);
    	bufferArray = createInputBufferArray(BUFFERQ_SIZE);
    	System.out.println("Size of InputDataBuffers:"+(mem1-Runtime.getRuntime().freeMemory())/Utils.mb+" mb" );
    }

    public long importNodesParallel(BufferedReader reader) throws IOException {   	
        final ChunkerLineData input = new ChunkerLineData(reader, config.getDelimChar(), 0);
        input.getChunker().setRecordType(RunData.NODE);
        //bufferQ = new NewArrayBlockingQ(IMPORT_NODE_STAGES, BUFFERQ_SIZE, IMPORT_NODE_THREADS, singleThreadedStages);
        bufferQ = new NewArrayBlockingQ(IMPORT_NODE_STAGES, BUFFERQ_SIZE, IMPORT_NODE_THREADS);
        initInputQ(bufferQ.getQ(0));//inputQ);
        db.setDataInput(input, bufferArray);
        input.setParallelImpl(bufferQ.getQ(0));//inputQ);
        stageComplete = new boolean[IMPORT_NODE_STAGES]; 
        stageRunData = new RunData[IMPORT_NODE_STAGES][IMPORT_NODE_THREADS];
        writerRunData = new RunData[3];
        try{
        	Class[] parameterTypes = new Class[2];
        	parameterTypes[0] = ChunkerLineData.class;
            parameterTypes[1] = DataBuffer.class;
            Method[] ImportNode_Stages = new Method[IMPORT_NODE_STAGES];
            for (int stageIndex = 0; stageIndex < IMPORT_NODE_STAGES; stageIndex++){
            	ImportNode_Stages[stageIndex] = ProcessBufferMethods.class.getMethod("ImportNode"+stageIndex, parameterTypes);
            	stageComplete[stageIndex] = false;
            	for (int i = 0; i < IMPORT_NODE_THREADS; i++)
            	stageRunData[stageIndex][i] = new RunData("NodeStage_"+ImportNode_Stages[stageIndex].getName()+"_"+i);
            	bufferQ.setSingleThreaded(stageIndex, singleThreadedStages[stageIndex]);
            }
            for (int i = 0; i < IMPORT_NODE_THREADS; i++){
            	importNodeWorkers[i] = new ImportWorker(input, ImportNode_Stages, i);
            	importNodeWorkers[i].start();
            }
            //start writers
            for (int i = RunData.PROPERTY; i <= RunData.NODE; i++){
            	//diskRecordsQ[i] = new ArrayBlockingQueue<DiskRecords>(NewImporter.BUFFERQ_SIZE); 
            	writerRunData[i] = new RunData("Worker_"+i);
            	writerWorkers[i] = new WriteWorker(i, i, "Writer"+i);
            	writerWorkers[i].start();
            }
        } catch (Exception e){
        	Utils.SystemOutPrintln("Method failed:"+e.getMessage());
        }
        report.reset();
        long startTime = System.currentTimeMillis();
        int waitCount = 0;
        while (true){
        	try {
        		Thread.sleep(500);
        		waitCount += 500;
        		if (waitCount % pollInterval== 0)
        			printRunData(detailedData, writerWorkers, IMPORT_NODE_STAGES, startTime);
        		if (activeThreadCount <= 0)
        			break;
        	} catch (Exception e){
        		Utils.SystemOutPrintln(e.getMessage()+" Writer exception");
        		return 0 ;
        	}
        }
        flushIndexes();
        System.out.println("Exiting main thread");
        System.out.println("Completed Node Import:"+(System.currentTimeMillis()-startTime)+" ms");
        return (System.currentTimeMillis()-startTime);
    }
    //============================Parallel Relationship logic
    boolean aggregateLogic = false;
    int IMPORT_RELATIONSHIP_STAGES_MAX = 5;
    int IMPORT_RELATIONSHIP_STAGES = 5;
    int IMPORT_RELATIONSHIP_THREADS = Runtime.getRuntime().availableProcessors();//IMPORT_RELATIONSHIP_STAGES*3;
    private boolean[] importRelationshipThreadType = new boolean[]{false,true,false,true,true,true,true};
    private int activeThreadCount = 0;
    private ImportWorker[] importRelationWorkers = new ImportWorker[IMPORT_RELATIONSHIP_THREADS];
    private  WriteWorker[] writerWorkers = new  WriteWorker[3];
    private Method[] ImportRelation_Stages = new Method[IMPORT_RELATIONSHIP_STAGES];
    private DataBuffer[] bufferArray;
    
    public long importRelationshipsParallel(BufferedReader reader) throws IOException {
    	final ChunkerLineData input = new ChunkerLineData(reader, config.getDelimChar(), 3);
    	input.getChunker().setRecordType(RunData.RELATIONSHIP);
    	bufferQ = new NewArrayBlockingQ(IMPORT_RELATIONSHIP_STAGES, BUFFERQ_SIZE, IMPORT_RELATIONSHIP_THREADS);
    	initInputQ(bufferQ.getQ(0));//inputQ);
    	db.setDataInput(input, bufferArray);
    	input.setParallelImpl(bufferQ.getQ(0));//inputQ);
        stageComplete = new boolean[IMPORT_RELATIONSHIP_STAGES]; 
        stageRunData = new RunData[IMPORT_RELATIONSHIP_STAGES][IMPORT_RELATIONSHIP_THREADS];
        writerRunData = new RunData[3];
      
        try{     	
        	
        	Class[] parameterTypes = new Class[2];
        	parameterTypes[0] = ChunkerLineData.class;
            parameterTypes[1] = DataBuffer.class;         
    		           
            for (int stageIndex = 0; stageIndex < IMPORT_RELATIONSHIP_STAGES; stageIndex++){
            	ImportRelation_Stages[stageIndex] = ProcessBufferMethods.class.getMethod("ImportRelation"+stageIndex, parameterTypes);
            	stageComplete[stageIndex] = false;
            	for (int i = 0; i < IMPORT_RELATIONSHIP_THREADS; i++)
            		stageRunData[stageIndex][i] = new RunData("RelStage_"+ImportRelation_Stages[stageIndex].getName()+"_"+i);
            	bufferQ.setSingleThreaded(stageIndex, importRelationshipThreadType[stageIndex]);
            }
          //start writers
            for (int i = RunData.PROPERTY; i <= RunData.RELATIONSHIP; i++){
            	//diskRecordsQ[i] = new ArrayBlockingQueue<DiskRecords>(NewImporter.BUFFERQ_SIZE); 
            	writerRunData[i] = new RunData("Worker_"+i);
            	writerWorkers[i] = new WriteWorker(i, i, "Writer"+i);         	
            	writerWorkers[i].start();
            }
            for (int threadIndex = 0; threadIndex < IMPORT_RELATIONSHIP_THREADS; threadIndex++){
            	importRelationWorkers[threadIndex] = new ImportWorker(input,ImportRelation_Stages, threadIndex);
            	Thread.sleep(100);
            	importRelationWorkers[threadIndex].start();
    		}


        } catch (Exception e){
        	Utils.SystemOutPrintln("ImportRelationParallel failed:"+e.getMessage());
    	}

        long startTime = System.currentTimeMillis();
        boolean threadAlive = true;
        int waitCount = 0;
        while (true){
        	try {
        		Thread.sleep(500);
        		waitCount += 500;
        		if (waitCount % pollInterval== 0)
        			printRunData(detailedData, writerWorkers, IMPORT_RELATIONSHIP_STAGES, startTime);
        		if (activeThreadCount <= 0)
        			break;
        	} catch (Exception e){
        		Utils.SystemOutPrintln("Import relationship main thread "+e.getMessage());
        		return 0;
        	}
        }
        //System.out.println("Exiting main thread");
        System.out.println("Completed Relationship Pass1:"+(System.currentTimeMillis()-startTime)+" ms");
		return (System.currentTimeMillis()-startTime);
    }

    private class ProcessBufferMethods{
    	//Extract
    	public void ImportRelation0(ChunkerLineData input, DataBuffer buf){
    		DataExtract(input, buf);
    	}
    	public void ImportNode0(ChunkerLineData input, DataBuffer buf){
    		DataExtract(input, buf);
    	}
    	public void DataExtract(ChunkerLineData input, DataBuffer buf){
    		try {
    			boolean hasMoreData = input.getChunker().fillBuffer(buf);
    			if (!hasMoreData)
    				buf.setMoreData(hasMoreData);  	
    		} catch (Exception ioe){
    			Utils.SystemOutPrintln("Bad input data-"+ioe.getMessage());
    		}
    	}

    	//Transform
    	public void ImportNode11(ChunkerLineData input, DataBuffer buf){
    		try {
    			if (!input.hasId())
    				db.assignNodeIds(buf);
    		} catch (Exception e){
    			Utils.SystemOutPrintln(e.getMessage()+"--ImportNode2");
    		}
    	}
    	public void ImportNode1(ChunkerLineData input, DataBuffer buf){
    		try {
    			if (!input.hasId()){
    				buf.perfData.switchClock("NodeId");
    				db.createNodeRecords(buf);
    			}
	    	//buf.perfData.switchClock("CreateProps");
    		//db.importNode_EncodeProps(buf);
    		} catch (Exception e){
    			Utils.SystemOutPrintln(e.getMessage()+"--ImportNode1");
    		}
    	}

    	public void ImportNode2(ChunkerLineData input, DataBuffer buf){
    		try {
    			buf.perfData.switchClock("CreateProps");
        		db.importNode_EncodeProps(buf);
        		} catch (Exception e){
        			Utils.SystemOutPrintln(e.getMessage()+"--ImportNode2");
        		}
    		//}
	    //public void ImportNode3(ChunkerLineData input, InputDataBuffer buf){
	    	try {
    		buf.perfData.switchClock("WritePrp");
    		db.createPropertyRecords(buf, false);
    		buf.perfData.switchClock("WriteNode");
    		db.importNode_writeNodeStore(buf, false);
	    	} catch (Exception e){
    			Utils.SystemOutPrintln(e.getMessage()+"--ImportNode3");
    		}
	    }
	    
	    public void ImportNode3(ChunkerLineData input, DataBuffer buf){
	    	addIndexValues(input, buf);//addIndexValuesAsProps(input, buf);
	    }
	    public void addIndexValues(ChunkerLineData input, DataBuffer buf){
	    	for (int i = 0; i < buf.curEntries; i++){
	    		buf.perfData.switchClock("GetIndex");
	    		Map<String, Map<String, Object>> entries = db.getDataInput().getIndexData(buf, i);	    		
	    		buf.perfData.switchClock("ProcessIndex");
	    		for (Map.Entry<String, Map<String, Object>> entry : entries.entrySet()) {
	    			String indexId = entry.getKey();
	    			Map<String, Object> value = entry.getValue();
	    			final BatchInserterIndex index = indexFor(indexId);
	    			if (index==null)
	    				throw new IllegalStateException("Index "+entry.getKey()+" not configured.");
	    			index.add(buf.id[i], entry.getValue());
	    		}
	    	}
	    }
	    public void addIndexValuesbyColumn(ChunkerLineData input, DataBuffer buf){
	    	for (int column = input.getOffset(); column < input.getHeaderLength(); column++){
	    		String indexName = input.getHeader()[column].indexName;
	    		if (indexName == null)
	    			continue;
	    		for (int i = 0; i < buf.curEntries; i++){
	    			Map<String, Object> entry = db.getDataInput().getIndexData(buf, i, column);	    		
	    			if (entry == null)
	    				continue;
	    			final BatchInserterIndex index = indexFor(indexName);
	    			if (index==null)
	    				throw new IllegalStateException("Index "+indexName+" not configured.");
	    			index.add(buf.id[i], entry);
	    		}
	    	}
	    }


	    public void ImportRelation1(ChunkerLineData input, DataBuffer buf){
	     	buf.perfData.switchClock("CreateRel");
	     	db.createRelationship1(buf);
	    }
	   
	    public void ImportRelation2(ChunkerLineData input, DataBuffer buf){
	    	buf.perfData.switchClock("encodeProps");
	    	db.createRelationship2(buf);
	    }
	    
	    public void ImportRelation3(ChunkerLineData input, DataBuffer buf){
	    	buf.perfData.switchClock("WriteProps");
	    	db.createPropertyRecords(buf, true);
	    }	
	    public void ImportRelation4(ChunkerLineData input, DataBuffer buf){
	    	buf.perfData.switchClock("ConnectUpdateRel");
	    	db.createRelationship4(buf);
	    }
	    
	    //Load
	    public void LoadData0(ChunkerLineData input, DiskRecords buf){
	    	db.writeProperty(buf);
	    }
	    public void LoadData1(ChunkerLineData input, DiskRecords buf){
	    	db.writeNode(buf);
	    }
	    public void LoadData2(ChunkerLineData input, DiskRecords buf){
	    	db.writeRelationship(buf);
	    }
    }

  
    private Map<String,Object> convertProps(Properties props){
    	if (props == null)
    		return null;
    	Map<String,Object> map = new HashMap<String,Object>();
    	Iterator it = props.keySet().iterator();
    	while (it.hasNext()){
    		String key = (String)it.next();
    		map.put(key, props.get(key));
    	}
    	return map;
    }
    class ImportWorker extends java.lang.Thread {
    	private Exception excep;
    	private String name = "";
    	//public RunData perfData = null;
    	private boolean isRunning = false;
		private Method[] importWorkerMethods;
		ChunkerLineData input;
		int stageIndex = -1;
		int threadIndex;
		
		ImportWorker(ChunkerLineData inp, Method[] importWorkerMethods, int threadIndex) {
			input = inp;
    		this.importWorkerMethods = importWorkerMethods;
    		this.threadIndex = threadIndex;
    	}
		
		public void resetImportWorkers(Method[] importWorkerMethods){
			this.importWorkerMethods = importWorkerMethods;
		}
    	public void run() {
    		activeThreadCount++;
    		isRunning = true;
    		//threadLocalPerf.set(perfData);
    		Thread.currentThread().setName(name);
    		DataBuffer buffer = null;
    		int bufOrder = -1;
    		
    		while (!isDone()) {
    			try {
    				if (stageIndex >= 0)
    					stageRunData[stageIndex][threadIndex].switchClock(RunData.QGET);
    				try {
    					buffer = bufferQ.getBuffer(stageIndex, buffer);
    				} catch (Exception e) {
    					Utils.SystemOutPrintln("Thread exception: in getBuffer");
    				}
    				if (buffer == null){
    					Thread.sleep(100);
    					continue;
    				}
    				stageIndex = buffer.stageIndex;
    				if (bufferQ.isSingleThreaded(stageIndex))
    					this.setPriority(Thread.NORM_PRIORITY+1);
    				else
    					this.setPriority(Thread.NORM_PRIORITY);
    				String methodName = importWorkerMethods[buffer.stageIndex].getName();
    	    		this.name = Thread.currentThread().getName() + "ImportNode_"+methodName;
    	    		threadLocalPerf.set(stageRunData[buffer.stageIndex][threadIndex]);
    				if (buffer != null){
    					threadLocal.set(buffer);
    					buffer.perfData = stageRunData[buffer.stageIndex][threadIndex];
    					Object[] parameters = new Object[2];
    					parameters[0] = this.input;
    					parameters[1] = buffer;
    					try{
    						importWorkerMethods[buffer.stageIndex].invoke(new ProcessBufferMethods(), parameters);
    					} catch (Exception e) {
    	    				this.excep = e;
    	    				Utils.SystemOutPrintln("Thread exception111:"+name+":" + e.getMessage());
    					}
    					stageRunData[buffer.stageIndex][threadIndex].linesProcessed += buffer.curEntries;
    					if (!buffer.isMoreData() && buffer.curEntries > 0) {
    						stageComplete[buffer.stageIndex] = true;
    						if (buffer.stageIndex == 0){
    							//wait till all the fellow threads in stage 0 to complete
    							while (bufferQ.threadCount[0] > 1)
    								Thread.sleep(100);
    						}
    						System.out.println("Stage ["+buffer.stageIndex+"] is complete");
    					}
    					stageRunData[buffer.stageIndex][threadIndex].switchClock(RunData.QPUT);
    					buffer = bufferQ.putBuffer(buffer);
    				}
    			} catch (Exception e) {
    				this.excep = e;
    				Utils.SystemOutPrintln("Thread exception:"+name+":" + e.getMessage());
    				isRunning = false;
    				//break;
    			} 
    		}
    		flushIndexes();
    		isRunning = false;
    		System.out.println("Thread:"+name+ " exiting "+activeThreadCount);
    		activeThreadCount--;
    	}
    	
    	public Exception getException() {
    		return this.excep;
    	}
    	
    	private boolean isRunning() {
    		return isRunning;
    	}

    	public boolean isDone(){
    		for (int stageIndex = 0; stageIndex < stageComplete.length; stageIndex++)
    			if (!stageComplete[stageIndex])
    				return false;
    		return true;
    	}
    	
    }
 
    private Label[] labelsFor(String[] labels) {
        if (labels == null || labels.length == 0) return NO_LABELS;
        if (labels.length != labelsArray.length) labelsArray = new Label[labels.length];
        for (int i = labels.length - 1; i >= 0; i--) {
            if (labelsArray[i] == null || !labelsArray[i].name().equals(labels[i]))
                labelsArray[i] = DynamicLabel.label(labels[i]);
        }
        return labelsArray;
    }

    private long lookup(String index,String property,Object value) {
        Long id = indexFor(index).get(property, value).getSingle();
        return id==null ? -1 : id;
    }

    private BatchInserterIndex indexFor(String index) {
        return indexes.get(index);
    }

    void importRelationships(BufferedReader reader) throws IOException {
        final int offset = 3;
        final LineData data = createLineData(reader, offset);
        final RelType relType = new RelType();
        long skipped=0;
        report.reset();
        RunData perfData = new RunData("NewImporter Root thread");
        threadLocalPerf.set(perfData);
        try {
        	while (data.processLine(null)) {
        		final Map<String, Object> properties = data.getProperties();
        		final long start = id(data, 0);
        		final long end = id(data, 1);
        		if (start==-1 || end==-1) {
        			skipped++;
        			continue;
        		}
        		final RelType type = relType.update(data.getRelationshipTypeLabel());

        		final long id = batchInserter.createRelationship(start, end, type, properties);
        		perfData.switchClock("MapRel");
        		for (Map.Entry<String, Map<String, Object>> entry : data.getIndexData().entrySet()) {
        			indexFor(entry.getKey()).add(id, entry.getValue());
        		}
        		perfData.switchClock("Misc");

        		report.dots();
        		perfData.printData(15);
        	}
        } catch (Exception e){
        	Utils.SystemOutPrintln("Createrelationship Failed:"+e.getMessage());
        }
        String msg = "Relationships";
        if (skipped > 0) msg += " skipped (" + skipped + ")";
        report.finishImport(msg);
    }


    private void flushIndexes() {
        for (BatchInserterIndex index : indexes.values()) {
            index.flush();
        }
    }

    private LineData createLineData(BufferedReader reader, int offset) {
        final boolean useQuotes = config.quotesEnabled();
        if (useQuotes) return new CsvLineData(reader, config.getDelimChar(),offset);
        return new ChunkerLineData(reader, config.getDelimChar(), offset);
    }

    private long id(LineData data, int column) {
        final LineData.Header header = data.getHeader()[column];
        final Object value = data.getValue(column);
        if (header.indexName == null || header.type == Type.ID) {
            return id(value);
        }
        return lookup(header.indexName, header.name, value);
    }

    void importIndex(String indexName, BatchInserterIndex index, BufferedReader reader) throws IOException {
        final LineData data = createLineData(reader, 1);
        report.reset();
        while (data.processLine(null)) {
            final Map<String, Object> properties = data.getProperties();
            index.add(id(data.getValue(0)), properties);
            report.dots();
        }
                
        report.finishImport("Done inserting into " + indexName + " Index");
    }

    private BatchInserterIndex nodeIndexFor(String indexName, String indexType) {
        return indexProvider.nodeIndex(indexName, configFor(indexType));
    }

    private BatchInserterIndex relationshipIndexFor(String indexName, String indexType) {
        return indexProvider.relationshipIndex(indexName, configFor(indexType));
    }

    private Map<String, String> configFor(String indexType) {
        if (indexType.equalsIgnoreCase("fulltext")) return FULLTEXT_CONFIG;
        if (indexType.equalsIgnoreCase("spatial")) return SPATIAL_CONFIG;
        return EXACT_CONFIG;
    }

    private long id(Object id) {
        return Long.parseLong(id.toString());
    }

    private void importIndex(IndexInfo indexInfo) throws IOException {
        File indexFile = new File(indexInfo.indexFileName);
        if (!indexFile.exists()) {
            System.err.println("Index file "+indexFile+" does not exist");
            return;
        }
        importIndex(indexInfo.indexName, indexes.get(indexInfo.indexName), createFileReader(indexFile));
    }

    private void doImport() throws IOException {
        try {
        	System.out.println("At start-"+Utils.memoryStats());
        	if (new_parallel_import_node || new_parallel_import_rel)
        		prepareForParallel();
        	System.out.println("After prepare and before node-"+Utils.memoryStats());
            for (File file : config.getNodesFiles()) {
                if (!new_parallel_import_node)
                	importNodes(createFileReader(file));
                else
                	importNodesParallel(createFileReader(file));
            }
            System.out.println("Highest Node Id:"+(batchInserter.getNeoStore().getNodeStore().getHighId()));
            System.out.println("Between node and relationship-"+Utils.memoryStats());
            if (maxNodes != -1)
            	maxNodes = (int)batchInserter.getNeoStore().getNodeStore().getHighId();
            if (new_parallel_import_node)
            	db.setNextRelBuffer();
            Thread.sleep(1000);
            long timeTaken = 0;
            for (File file : config.getRelsFiles()) {
            	while (true) {
            		try {
            			if (!new_parallel_import_rel)
            				importRelationships(createFileReader(file));
            			else
            				timeTaken = importRelationshipsParallel(createFileReader(file));
            			break;
            		} catch (Exception e){
            			Utils.SystemOutPrintln("Import Relations Failed:"+e.getMessage());
            			break;
            		}
            	}
            }
            long startTimer = System.currentTimeMillis();
            if (new_parallel_import_rel){            	
            	int errorCount = db.writeNodeNextRel();
            	System.out.println("WriteNodes completed in "+(System.currentTimeMillis()-startTimer)+" ms with "+errorCount+" errors");
            	timeTaken += (System.currentTimeMillis()-startTimer);
            	//startTimer = System.currentTimeMillis();
            	//errorCount = linkForward(db.getNeoStore());
            	//System.out.println("Link Forward complete in "+(System.currentTimeMillis()-startTimer)+" ms with "+errorCount+" errors");
            	startTimer = System.currentTimeMillis();
            	errorCount = linkBack(batchInserter.getNeoStore());
            	System.out.println("Link back complete in "+(System.currentTimeMillis()-startTimer)+" ms with "+errorCount+" errors");
            	timeTaken += (System.currentTimeMillis()-startTimer);
            }
           
            for (IndexInfo indexInfo : config.getIndexInfos()) {
                if (indexInfo.shouldImportFile()) importIndex(indexInfo);
            }
            System.out.println("Import of Relationships complete in: "+timeTaken/1000+" s");
		} catch (Exception e){
			Utils.SystemOutPrintln("doImport failed:"+e.getMessage());
		}
        finally {
            finish();
        }
    }


    final static int BUFFERED_READER_BUFFER = 4096*512;

    private BufferedReader createFileReader(File file) {
        try {
            final String fileName = file.getName();
            if (fileName.endsWith(".gz") || fileName.endsWith(".zip")) {
            	InputStreamReader inp = new InputStreamReader(new GZIPInputStream(new BufferedInputStream(new FileInputStream(file)),BUFFERED_READER_BUFFER));
            	return new BufferedReader(inp);
            }
            final FileReader fileReader = new FileReader(file);
            return new BufferedReader(fileReader,BUFFERED_READER_BUFFER);
        } catch(Exception e) {
            throw new IllegalArgumentException("Error reading file "+file+" "+e.getMessage(),e);
        }
    }
    
    
    class WriteWorker extends java.lang.Thread {
    	int inputQIndex;
    	int workerType;
    	String threadName ;
    	RunData perfData;
    	WriteWorker(int type,  int qIndex, String name){
    		inputQIndex = qIndex;
    		workerType = type;
    		threadName = name;
    		perfData = new RunData(name);
    	    threadLocalPerf.set(perfData);
    	}
    	public void run() {
    		Thread.currentThread().setName(threadName);
    		this.setPriority(NORM_PRIORITY+2);
    		threadLocalPerf.set(writerRunData[workerType]);
    		while (true){
    			try {
    				DiskRecords buf = diskRecordsQ.getBuffer(inputQIndex);//inputQ.take(); 
    				if (workerType == RunData.PROPERTY)
    					db.writeProperty(buf);
    				else if (workerType == RunData.NODE)
    					db.writeNode(buf);
    				else if (workerType == RunData.RELATIONSHIP)
    					db.writeRelationship(buf);
    				writerRunData[workerType].linesProcessed += buf.curEntries;
    				if (buf.isLastBuffer())
    					break;	
    				diskRecordsCache.putDiskRecords(buf, inputQIndex);
    			} catch (Exception e){
    				Utils.SystemOutPrintln("Writer died");
    				break;
    			}
    		}
    		System.out.println("Exiting writer "+ threadName);
    	}
    }
    private String getMaxIds(){
    	long[] highIds = new long[4];
    	highIds[0] = batchInserter.getNeoStore().getPropertyStore().getHighId();
    	highIds[1] = batchInserter.getNeoStore().getNodeStore().getHighId();
    	highIds[2] = batchInserter.getNeoStore().getRelationshipStore().getHighId();
    	highIds[3] = batchInserter.getNeoStore().getLabelTokenStore().getHighId();
    	return ("Property["+highIds[0]+"] Node["+highIds[1]+"] Relationship["+highIds[2]+"] Label["+highIds[3]+"]");
    }
	public void printRunData(boolean detailed, WriteWorker[] writers, int stages, long startTime){
		long curTime = System.currentTimeMillis();		
		long timeTaken = (curTime-startTime);
		System.out.println("At time ["+(curTime-startImport)+" ms]["+timeTaken+" ms]");
		System.out.println("\t"+Utils.memoryStats());
		StringBuilder str = new StringBuilder();
		str.append("Q Stats");
		for (int i = 0; i < bufferQ.getLength();i++)
			str.append("["+bufferQ.getQ(i).size()+":"+bufferQ.threadCount[i]+"]");
		str.append("]");		
		
		System.out.println("\t"+str);
		str.setLength(0);
		str.append("StageStats["+timeTaken+" ms]");
		for (int j = 0; j < stages; j++){
			long linesProcessed = 0; 
			long snapRate = 1, rate = 1;;
			if (stageComplete[j])
				continue;		
    		for (int i = 0; i < stageRunData[j].length && stageRunData[j][i] != null; i++){
				linesProcessed += stageRunData[j][i].linesProcessed;
				rate += stageRunData[j][i].totalRate();
				snapRate += stageRunData[j][i].snapshotRate();
			}
			String type = "M";
			if (bufferQ.isSingleThreaded(j))
				type = "S";
			//System.out.println(stageRunData[j][0].name+"-"+type);
			//str = new StringBuilder();
			//str.append("\tCumulative:["+timeTaken+"] ms for ["+linesProcessed+"] records at total ["+rate+"] records/sec with current rate ["+snapRate+"] records/sec");
			//System.out.println(str);
			str.append("["+type+":"+linesProcessed+":"+rate+":"+snapRate+"]");
			if (detailed)
				for (int i = 0; i < stageRunData[j].length && stageRunData[j][i] != null; i++)
						stageRunData[j][i].printData(0);
		};
		System.out.println("\t"+str);
		System.out.println("\t"+"MaxIds:"+getMaxIds());
	}
    public static class ObjectSizeFetcher {
        private static Instrumentation instrumentation;

        public static void premain(String args, Instrumentation inst) {
            instrumentation = inst;
        }

        public static long getObjectSize(Object o) {
            return instrumentation.getObjectSize(o);
        }
    }
    
	public int linkBackWithErrorCheck(NeoStore neoStore){
		int errorCount = 0;
		try {
		long maxNodeId = neoStore.getNodeStore().getHighId();
		long maxRelId = neoStore.getRelationshipStore().getHighId();
		System.out.println(Utils.memoryStats());
		long[][] relCache = new long[2][(int)maxNodeId];
		Arrays.fill(relCache[0], -1);
		Arrays.fill(relCache[1], -1);
		System.out.println(Utils.memoryStats());
		long startLinkBack = System.currentTimeMillis();
		for (long id = maxRelId-1; id >= 0; id--){
			RelationshipRecord relRecord =  neoStore.getRelationshipStore().getRecord(id);
			long relId = relRecord.getId();
			int firstNode = (int)relRecord.getFirstNode();
			int secondNode = (int)relRecord.getSecondNode();
			if (relCache[1][firstNode] != -1 && relCache[1][firstNode] != relId){
				System.out.println("Error: [firstNode]["+firstNode+"][relCache[1][firstNode]["+relCache[1][firstNode]+"] relId["+relId+"]");
				errorCount++;
			}
			if (relCache[1][secondNode] != -1 && relCache[1][secondNode] != relId){
				errorCount++;
				System.out.println("Error: [secondNode]["+secondNode+"][relCache[1][secondNode]["+relCache[1][secondNode]+"] relId["+relId+"]");
			}
			relRecord.setFirstPrevRel(relCache[0][firstNode]);
			relRecord.setSecondPrevRel(relCache[0][secondNode]);
			//long firstPrevRel = nodePrevRel[(int)records[i].getFirstNode()];
			//long secondPrevRel = nodePrevRel[(int)records[i].getSecondNode()];
			//nodePrevRel[(int)records[i].getFirstNode()] = records[i].getId();
			//nodePrevRel[(int)records[i].getSecondNode()] = records[i].getId();
			relCache[0][firstNode] = relId;
			relCache[0][secondNode] = relId;
			neoStore.getRelationshipStore().updateRecord(relRecord);
			if (id % 10000000 == 0)
				System.out.println("Completed linking "+(maxRelId-id)+ " links in "+(System.currentTimeMillis()-startLinkBack)+" ms");
		}
		} catch (Exception e){
			Utils.SystemOutPrintln("Error in link back logic");
		}
		return errorCount;
	}
	public int linkBack(NeoStore neoStore){
		int errorCount = 0;
		long current = 0, prev = 0;
		long maxNodeId = neoStore.getNodeStore().getHighId();
		long maxRelId = neoStore.getRelationshipStore().getHighId();
		System.out.println(Utils.memoryStats());
		long[] relCache = db.setNextRelBuffer(maxNodeId);
		System.out.println(Utils.memoryStats());
		long startLinkBack = System.currentTimeMillis();
		for (long id = maxRelId-1; id >= 0; id--){
			try {
				RelationshipRecord relRecord =  neoStore.getRelationshipStore().getRecord(id);
				long relId = relRecord.getId();
				int firstNode = (int)relRecord.getFirstNode();
				int secondNode = (int)relRecord.getSecondNode();
				if (relRecord.getFirstPrevRel() != relCache[firstNode] || 
						relRecord.getSecondPrevRel() != relCache[secondNode]){
					relRecord.setFirstPrevRel(relCache[firstNode]);
					relRecord.setSecondPrevRel(relCache[secondNode]);
					neoStore.getRelationshipStore().updateRecord(relRecord);
				}
				//long firstPrevRel = nodePrevRel[(int)records[i].getFirstNode()];
				//long secondPrevRel = nodePrevRel[(int)records[i].getSecondNode()];
				//nodePrevRel[(int)records[i].getFirstNode()] = records[i].getId();
				//nodePrevRel[(int)records[i].getSecondNode()] = records[i].getId();
				relCache[firstNode] = relId;
				relCache[secondNode] = relId;						
				if (id % 10000000 == 0){
					current = System.currentTimeMillis();
					System.out.println("Completed linking "+(maxRelId-id)+ " links in "+(current-startLinkBack)+" ms ["+(current-prev)+"]");
					prev = current;
				}
			} catch (Exception e){
				Utils.SystemOutPrintln("Error in link back logic "+ e.getMessage());
			}
		}
		return errorCount;
	}
	public int linkForward(NeoStore neoStore){
		int errorCount = 0;
		long firstNextRel, secondNextRel, firstNodeId, secondNodeId, current = 0, prev = 0;
    	RelationshipRecord rel;
    	
		long maxNodeId = neoStore.getNodeStore().getHighId();
		long maxRelId = neoStore.getRelationshipStore().getHighId();
		System.out.println(Utils.memoryStats());
		long[] nodeNextRel = db.setNextRelBuffer(maxNodeId);
		System.out.println(Utils.memoryStats());
		long startLinkBack = System.currentTimeMillis();
		//for (long id = maxRelId-1; id >= 0; id--){
		for (long id = 0; id < maxRelId; id++){
			try {
				rel =  neoStore.getRelationshipStore().getRecord(id);
				firstNodeId = (int)rel.getFirstNode();
				secondNodeId = (int)rel.getSecondNode();
				
				if (firstNodeId == secondNodeId){
    				firstNextRel = secondNextRel = nodeNextRel[(int)firstNodeId];
    				nodeNextRel[(int)firstNodeId] = rel.getId();
    			} else {
    				firstNextRel  = nodeNextRel[(int)firstNodeId];	
    				nodeNextRel[(int)firstNodeId] = rel.getId();
    				secondNextRel = nodeNextRel[(int)secondNodeId];
    				nodeNextRel[(int)secondNodeId] = rel.getId();
    			}
    			assert firstNextRel != rel.getId();
    			assert secondNextRel != rel.getId();
    			if (rel.getFirstNextRel() != firstNextRel || rel.getSecondNextRel() != secondNextRel){
    				rel.setFirstNextRel( firstNextRel );
    				rel.setSecondNextRel( secondNextRel );

    				neoStore.getRelationshipStore().updateRecord(rel);
    			}
				if (id % 10000000 == 0){
					current = System.currentTimeMillis();
					System.out.println("Completed linking "+(id)+ " links in "+(current-startLinkBack)+" ms ["+(current-prev)+"]");
					prev = current;
				}

			} catch (Exception e){
				Utils.SystemOutPrintln("Error in link back logic "+ e.getMessage());
			}
		}
		return errorCount;
	}
}